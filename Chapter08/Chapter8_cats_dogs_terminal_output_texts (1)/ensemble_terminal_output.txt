    result = self.forward(*input, **kwargs)
  File "/home/greg/.virtualenvs/ml/lib/python3.6/site-packages/torch/nn/modules/container.py", line 92, in forward
    input = module(input)
  File "/home/greg/.virtualenvs/ml/lib/python3.6/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/greg/.virtualenvs/ml/lib/python3.6/site-packages/torchvision/models/densenet.py", line 74, in forward
    new_features = layer(*features)
  File "/home/greg/.virtualenvs/ml/lib/python3.6/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/greg/.virtualenvs/ml/lib/python3.6/site-packages/torchvision/models/densenet.py", line 50, in forward
    bottleneck_output = bn_function(*prev_features)
  File "/home/greg/.virtualenvs/ml/lib/python3.6/site-packages/torchvision/models/densenet.py", line 22, in bn_function
    concated_features = torch.cat(inputs, 1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.95 GiB total capacity; 3.19 GiB already allocated; 6.69 MiB free; 70.83 MiB cached)
(ml) greg@earth:~/Desktop/packt/B11764_Hands-On_GAN/Chapter 08-Break_Others/provided_code/cats_dogs$ clear

(ml) greg@earth:~/Desktop/packt/B11764_Hands-On_GAN/Chapter 08-Break_Others/provided_code/cats_dogs$ python cats_dogs.py --train_single False --train_ensemble True
Image loader backend: PIL
Logging to output/log.txt

PyTorch version: 1.3.1
CUDA version: 10.1.243

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  resnet18
  cuda                |  bool      |  True
  train_single        |  bool      |  False
  train_ensemble      |  bool      |  True
  model_dir           |  str       |  ./models
  data_dir            |  str       |  ./cats-dogs-kaggle
  data_split          |  float     |  0.8
  cutout              |  bool      |  False
  cutout_length       |  int       |  64
  out_dir             |  str       |  output
  epochs              |  int       |  10
  batch_size          |  int       |  16
  lr                  |  float     |  0.01
  classes             |  int       |  2
  img_size            |  int       |  224
  channels            |  int       |  3
  log_interval        |  int       |  50
  pretrained_epoch    |  int       |  10
  seed                |  int       |  1
Loading data...

Loading model...

Training ensemble model...

Epoch 0, lr: 0.01
[0/25000]	loss: 1.3945	batch accuracy: 31.2500%
[800/25000]	loss: 0.0000	batch accuracy: 100.0000%
[1600/25000]	loss: 0.1495	batch accuracy: 93.7500%
[2400/25000]	loss: 0.0344	batch accuracy: 100.0000%
[3200/25000]	loss: 0.0853	batch accuracy: 100.0000%
[4000/25000]	loss: 0.0325	batch accuracy: 100.0000%
[4800/25000]	loss: 0.0218	batch accuracy: 100.0000%
[5600/25000]	loss: 0.0062	batch accuracy: 100.0000%
[6400/25000]	loss: 0.0142	batch accuracy: 100.0000%
[7200/25000]	loss: 0.0755	batch accuracy: 93.7500%
[8000/25000]	loss: 0.0005	batch accuracy: 100.0000%
[8800/25000]	loss: 0.0210	batch accuracy: 100.0000%
[9600/25000]	loss: 0.0694	batch accuracy: 100.0000%
[10400/25000]	loss: 0.2502	batch accuracy: 87.5000%
[11200/25000]	loss: 0.0854	batch accuracy: 100.0000%
[12000/25000]	loss: 0.0172	batch accuracy: 100.0000%
[12800/25000]	loss: 0.0850	batch accuracy: 93.7500%
[13600/25000]	loss: 0.2959	batch accuracy: 87.5000%
[14400/25000]	loss: 0.3216	batch accuracy: 93.7500%
[15200/25000]	loss: 0.0021	batch accuracy: 100.0000%
[16000/25000]	loss: 0.0327	batch accuracy: 100.0000%
[16800/25000]	loss: 0.0002	batch accuracy: 100.0000%
[17600/25000]	loss: 0.0156	batch accuracy: 100.0000%
[18400/25000]	loss: 0.1869	batch accuracy: 93.7500%
[19200/25000]	loss: 0.0208	batch accuracy: 100.0000%
Eval loss: 0.0281, accuracy: 99.1200
Epoch 1, lr: 0.001
[0/25000]	loss: 0.0100	batch accuracy: 100.0000%
[800/25000]	loss: 0.0120	batch accuracy: 100.0000%
[1600/25000]	loss: 0.4165	batch accuracy: 81.2500%
[2400/25000]	loss: 0.0219	batch accuracy: 100.0000%
[3200/25000]	loss: 0.0144	batch accuracy: 100.0000%
[4000/25000]	loss: 0.0059	batch accuracy: 100.0000%
[4800/25000]	loss: 0.0011	batch accuracy: 100.0000%
[5600/25000]	loss: 0.0010	batch accuracy: 100.0000%
[6400/25000]	loss: 0.0015	batch accuracy: 100.0000%
[7200/25000]	loss: 0.0186	batch accuracy: 100.0000%
[8000/25000]	loss: 0.0141	batch accuracy: 100.0000%
[8800/25000]	loss: 0.0216	batch accuracy: 100.0000%
[9600/25000]	loss: 0.0346	batch accuracy: 100.0000%
[10400/25000]	loss: 0.1247	batch accuracy: 93.7500%
[11200/25000]	loss: 0.0235	batch accuracy: 100.0000%
[12000/25000]	loss: 0.0102	batch accuracy: 100.0000%
[12800/25000]	loss: 0.0141	batch accuracy: 100.0000%
[13600/25000]	loss: 0.1599	batch accuracy: 93.7500%
[14400/25000]	loss: 0.0195	batch accuracy: 100.0000%
[15200/25000]	loss: 0.0132	batch accuracy: 100.0000%
[16000/25000]	loss: 0.1851	batch accuracy: 87.5000%
[16800/25000]	loss: 0.0122	batch accuracy: 100.0000%
[17600/25000]	loss: 0.0582	batch accuracy: 100.0000%
[18400/25000]	loss: 0.0175	batch accuracy: 100.0000%
[19200/25000]	loss: 0.0013	batch accuracy: 100.0000%
Eval loss: 0.0270, accuracy: 99.1600

Training GAN for adversarial attack...

epoch 1:
loss_D: 0.096, loss_G_fake: 0.726,             
loss_perturb: 23.466, loss_adv: 1.470, 

epoch 2:
loss_D: 0.005, loss_G_fake: 0.943,             
loss_perturb: 19.740, loss_adv: 1.223, 

epoch 3:
loss_D: 0.002, loss_G_fake: 0.971,             
loss_perturb: 18.843, loss_adv: 1.119, 

epoch 4:
loss_D: 0.001, loss_G_fake: 0.983,             
loss_perturb: 18.586, loss_adv: 1.070, 

epoch 5:
loss_D: 0.001, loss_G_fake: 0.986,             
loss_perturb: 18.160, loss_adv: 1.036, 

epoch 6:
loss_D: 0.000, loss_G_fake: 0.991,             
loss_perturb: 17.473, loss_adv: 0.984, 

epoch 7:
loss_D: 0.000, loss_G_fake: 0.995,             
loss_perturb: 17.028, loss_adv: 0.947, 

epoch 8:
loss_D: 0.001, loss_G_fake: 0.992,             
loss_perturb: 16.727, loss_adv: 0.945, 

epoch 9:
loss_D: 0.000, loss_G_fake: 0.993,             
loss_perturb: 16.495, loss_adv: 0.888, 

epoch 10:
loss_D: 0.000, loss_G_fake: 0.996,             
loss_perturb: 16.161, loss_adv: 0.887, 

Attacking ensemble model...

Eval loss: 0.8750, accuracy: 40.7000

