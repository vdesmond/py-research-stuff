{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reasonable-practice",
   "metadata": {},
   "source": [
    "# Using Python for Research: Final Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this final project, I'm tasked to predict the type of physical activity (e.g., walking, climbing stairs, sitting) from tri-axial smartphone accelerometer data. The dataset is split into 4 CSV files containing train_time_series, train_labels, test_time_series, test_labels respectively. The time series data is presented in the following format:\n",
    "`timestamp, UTC time, accuracy, x, y, z`\n",
    "and the labels are provided as:\n",
    "`timestamp, label`. However the test_labels don't contain the labels and the task is to predict and fill them.\n",
    "\n",
    "### Summary of steps performed in this task\n",
    "\n",
    " - First I read the files to pandas dataframes and then cleaned up the data.\n",
    " - I studied the data using box plot and pair plot from Seaborn. \n",
    " - After looking at the plots, I decided to use rolling mean and standard deviation of the time series data as covariates along with the x, y and z variates. \n",
    " - I used Random Forest Regression from `scikit-learn` to solve the task at hand. \n",
    " - I studied about various neural network architectures are used to solve time series classification problems, but still felt that a simple Random Forest Classifier would be sufficient to gain good accuracy. \n",
    " - The test labels are then predicted using the now fitted `forest_classifier` and stored in a CSV file. \n",
    " - An additional code block is given at the end for runtime calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-stationery",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Step 1\n",
    "\n",
    "First, we will import some libraries that will help in solving the task at hand. \n",
    " - `scikit-learn` (**sklearn**) contains helpful statistical models, in particular `RandomForestClassifier` that we'll use for classification and also some additional helper functions like `train_test_split`, `cross_val_score`, `confusion_matrix`, `classification_report`\n",
    " - `matplotlib.pyplot` and `seaborn` libraries are used here for data and metric visualizations.\n",
    " - `numpy` and `pandas` for data manipulation (obviously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-gazette",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Using `pd.read_csv`, the CSV files are read into dataframes of the same name. `index_col=0` is used to set the first column in csv as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_series = pd.read_csv(\"train_time_series.csv\", index_col=0)\n",
    "train_labels = pd.read_csv(\"train_labels.csv\", index_col=0)\n",
    "test_time_series = pd.read_csv(\"test_time_series.csv\", index_col=0)\n",
    "test_labels = pd.read_csv(\"test_labels.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of time series data: %d and Length of labelled data: %d\" % (len(train_time_series), len(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-riding",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "As seen above the raw data and actual labelled data are not equal. This is because the labels are only provided for every 10th observation. We can just extract the raw data to match the labels,but then the data would be quite low. Since we know that the labels don't change suddenly, it is possible to augment the data with preceding values. It is not perfect, but it is better than dealing with very low data.\n",
    "\n",
    "So we reindex the train labels with `reindex` method using new indices generated by `range` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = list(range(train_labels.index[0], train_labels.index[-1] + 1))\n",
    "train_labels = train_labels.reindex(new_index)\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-horse",
   "metadata": {},
   "source": [
    "### Step 4 \n",
    "\n",
    "Before filling the NaN values in train labels, we'll deal with the time series data first. Since the data is time series data, it is better if we consider not only the current value but also the influence of past values. A simple way to approach this would be to implement a **sliding window** (also known as **rolling window**).\n",
    "\n",
    "![sliding window](https://www.splunk.com/content/dam/splunk-blogs/signalfx-assets/blog-images//incremental-decremental.png)\n",
    "\n",
    "We will make use of the `rolling` method to find the *mean* and *standard deviation* of rolling window of size **100**. The window size is just a heuristic and was chosen by trial-and-error basis. The first value of `std` columns would be NaN, so `fillna` method is used with `method=bfill`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_series[[\"std_x\", \"std_y\", \"std_z\"]] = train_time_series[[\"x\", \"y\", \"z\"]].rolling(window=100, min_periods= 1).std()\n",
    "train_time_series[[\"mean_x\", \"mean_y\", \"mean_z\"]] = train_time_series[[\"x\", \"y\", \"z\"]].rolling(window=100, min_periods =1).mean()\n",
    "train_time_series.fillna(method=\"bfill\", inplace=True)\n",
    "train_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-pierre",
   "metadata": {},
   "source": [
    "### Step 4 \n",
    "\n",
    "Now that we got our required covariates, it is time to create our `X` and `y` dataframes which will be the arguments for the Random Forest Classifier. We also fill the NaN labels in y using the same `fillna` method seen above, but with `method=ffill` indicating forward fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = [\"x\",\"y\",\"z\",\"std_x\", \"std_y\", \"std_z\", \"mean_x\", \"mean_y\", \"mean_z\"]\n",
    "\n",
    "# Note that time_series data starts at index 20586 but labels start at 20589\n",
    "# We will splice out the covariates only from 20589 by leaving out first 3 indices\n",
    "X = train_time_series[covariates][3:]\n",
    "y = train_labels[\"label\"]\n",
    "y.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skew of covariates\n",
    "X.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-evaluation",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "For data visualization, we will make use of the seaborn library. First we'll concatenate our `X` and `y` dataframes to create `Xy`. Then we will create a 3x3 plotting grid for visualizing the data distributuon of our 9 covariates. Finally we make use of `boxenplot` function to plot enhanced box plots. The plot provides more information about the nature of distribution of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use pd.concat function with axis=1 for column-wise concatenation\n",
    "Xy = pd.concat([X,y],axis=1) \n",
    "fig, axes = plt.subplots(3,3, figsize=(15,15))\n",
    "for index,cov in enumerate(covariates):\n",
    "    i,j = index // 3, index % 3; # for getting the resepctive axes\n",
    "    sns.boxenplot(x=\"label\", y=cov, data=Xy, ax=axes[i][j])\n",
    "fig.suptitle(\"Boxplot of covariates\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-going",
   "metadata": {},
   "source": [
    "From the above plots we can see that the x, y and z boxplots have almost the same IQR for every class and follow fairly standard normal distibution. But looking at the rolling mean and standard deviation boxplots, we see greater variability in the median line and IQR, thus we can use these to achieve high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-richmond",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "Now we'll plot the marginal distribution of the univariates and pairwise scatter plot of the covariates using `pairplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-waste",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(Xy[[\"x\",\"y\",\"z\",\"label\"]], hue=\"label\", palette = \"hls\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(Xy[[\"std_x\",\"std_y\",\"std_z\",\"label\"]], hue=\"label\", palette = \"hls\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-resident",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(Xy[[\"mean_x\",\"mean_y\",\"mean_z\",\"label\"]], hue=\"label\", palette = \"hls\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-acrobat",
   "metadata": {},
   "source": [
    "By looking at the pairplot of x, y and z, we can infer that data has low bias and high variance. The standard deviation pairplot has noticable right skew, indicating positive correlation. The mean pairplot has slight skew, and some clusters can be seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-allowance",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "\n",
    "Finally we'll plot the correlation matrix using `heatmap` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-france",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(Xy.corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-closer",
   "metadata": {},
   "source": [
    "The correlation matrix further shows that the rolling standard deviation has strong positive correlation and also the rolling mean is also correlated with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-christian",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "\n",
    "Now it is time to train the classifier. But first we'll split our training data into `train` and `validation` sets with 80/20 split. We'll make use of scikit-learn's `train_test_split` function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of training split: %d\\nLength of validation split: %d\" % (len(X_train), len(X_validation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-albuquerque",
   "metadata": {},
   "source": [
    "### Step 9\n",
    "\n",
    "We'll create an instance of the classifier using `RandomForestClassifier()`. To find the best hyperparameters for our classifier, we'll use the `RandomizedSearchCV` function from scikit-learn. This will help us estimate the `n_estimators` and `max_depth`. Then we'll use the parameters to fit our data \n",
    "\n",
    "Note that the default arguments for `RandomForestClassifier()` work pretty well and hence this hyperparameter serach is not necessary, but it's good practice to search for optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# We'll search 10 n_estimators from 200 to 2000 and 12 max_depth parameters from 10 to 120 (along with default `None`)\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 120, num = 12)]\n",
    "max_depth.append(None)\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# We'll search for 100 candidates with 3 fold cross validation\n",
    "# This is a time consuming process, so setting n_jobs = -1 will make use of all the available cores.\n",
    "\n",
    "forest_classifier = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, n_jobs = -1)\n",
    "forest_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-operator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_classifier.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-intake",
   "metadata": {},
   "source": [
    "This shows the best hyperparameters obtained after `RandomizedSearchCV`. We'll use these best parameters directly next time (while calculating runtime). Now that the fitting is done, it is time to evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-bleeding",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We'll now score our classifier with the validation set using the `score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-batman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_classifier.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-fiction",
   "metadata": {},
   "source": [
    "Time to get the classification report and the confusion matrix plot using functions of the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_report function expects prediction output as argument, so we use `predict` method first\n",
    "\n",
    "y_validation_predicted = forest_classifier.predict(X_validation)\n",
    "class_report = classification_report(y_validation_predicted, y_validation)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(forest_classifier, X_validation, y_validation);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-ranking",
   "metadata": {},
   "source": [
    "The classifier has high accuracy, great F1 score and the confusion matrix looks good with very little misclassified labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-creator",
   "metadata": {},
   "source": [
    "Now we'll predict the labels for the test dataset provided. We add the rolling mean and standard deviation columns similar to what we did for train_time_series, and then predict the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = test_time_series[[\"x\",\"y\",\"z\"]]\n",
    "testX[[\"std_x\", \"std_y\", \"std_z\"]] = test_time_series[[\"x\",\"y\",\"z\"]].rolling(window=100, min_periods=1).std().fillna(method=\"bfill\")\n",
    "testX[[\"mean_x\", \"mean_y\", \"mean_z\"]] = test_time_series[[\"x\",\"y\",\"z\"]].rolling(window=100, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = forest_classifier.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-uniform",
   "metadata": {},
   "source": [
    "Similar to the train_labels, test_labels are only required for every 10th observation. So we splice the indices as required and store them in the `label` column. This could also be done by taking the mean/mode of every 10 predictions. We cast them as type `int` as that is how it was in the original train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[\"label\"] = testY[9::10].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-laser",
   "metadata": {},
   "source": [
    "Finally, we export the test_labels dataframe as *test_labels_filled.csv* using pandas' `to_csv` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.to_csv(\"test_labels_filled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-status",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have seen how a Random Forest Classifier can be used to classify time series data. \n",
    " - First we imported the data as pandas dataframes, then we filled in the missing gaps in labels using  `ffill`.\n",
    " - We used rolling window method to make use of the time-dependent nature of data, and created 6 additional covariates `std` and `mean` for each x, y and z. \n",
    " - We studied these covariates using `boxenplot`, `pairplot` and a correlation heatmap. These plots displayed the high positive correlation in the rolling standard_deviation and also the correlation between the rolling mean and labels.\n",
    " - We split our dataset into train and validation (80/20 split)\n",
    " - We used RandomizedSearchCV to estimate the best values for the hyperparameters `n_estimators` and `max_depth`\n",
    " - We then created a random forest classifer instance to fit our training data.\n",
    " - We evaluated our classifier using our validation data and obtained our classification report and confusion matrix, all of which showed great accuracy and F1 score.\n",
    " - Finally we used the classifier to predict the labels of given test_time_series data and exported the predictions to a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-division",
   "metadata": {},
   "source": [
    "## Runtime Calculation\n",
    "\n",
    "using the `%%time` magic command from IPython, we'll calculate the runtime of the code given, which excludes data visualization, metrics, hyperparameter search etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "train_time_series = pd.read_csv(\"train_time_series.csv\", index_col=0)\n",
    "train_labels = pd.read_csv(\"train_labels.csv\", index_col=0)\n",
    "test_time_series = pd.read_csv(\"test_time_series.csv\", index_col=0)\n",
    "test_labels = pd.read_csv(\"test_labels.csv\", index_col=0)\n",
    "\n",
    "new_index = list(range(train_labels.index[0], train_labels.index[-1] + 1))\n",
    "train_labels = train_labels.reindex(new_index)\n",
    "\n",
    "train_time_series[[\"std_x\", \"std_y\", \"std_z\"]] = train_time_series[[\"x\", \"y\", \"z\"]].rolling(window=100, min_periods= 1).std()\n",
    "train_time_series[[\"mean_x\", \"mean_y\", \"mean_z\"]] = train_time_series[[\"x\", \"y\", \"z\"]].rolling(window=100, min_periods =1).mean()\n",
    "train_time_series.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "covariates = [\"x\",\"y\",\"z\",\"std_x\", \"std_y\", \"std_z\", \"mean_x\", \"mean_y\", \"mean_z\"]\n",
    "X = train_time_series[covariates][3:]\n",
    "y = train_labels[\"label\"]\n",
    "y.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "forest_classifier = RandomForestClassifier(max_depth=None, n_estimators=800)\n",
    "forest_classifier.fit(X_train, y_train)\n",
    "print(forest_classifier.score(X_validation, y_validation))\n",
    "\n",
    "testX = test_time_series[[\"x\",\"y\",\"z\"]]\n",
    "testX[[\"std_x\", \"std_y\", \"std_z\"]] = test_time_series[[\"x\",\"y\",\"z\"]].rolling(window=100, min_periods=1).std().fillna(method=\"bfill\")\n",
    "testX[[\"mean_x\", \"mean_y\", \"mean_z\"]] = test_time_series[[\"x\",\"y\",\"z\"]].rolling(window=100, min_periods=1).mean()\n",
    "\n",
    "testY = forest_classifier.predict(testX)\n",
    "test_labels[\"label\"] = testY[9::10].astype(int)\n",
    "test_labels.to_csv(\"test_labels_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-plenty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edx",
   "language": "python",
   "name": "edx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
